---
title: "Machine Learning Hw4"
author: "Ekta Chaudhary"
date: "20/04/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(ISLR)
library(caret)
library(rpart)
library(rpart.plot)
library(party)
library(partykit)
library(randomForest)
library(ranger)
library(gbm)
library(plotmo)
library(pdp)
library(lime)
library(lasso2)
```

**(a) Fit a regression tree with lpsa as the response and the other variables as predictors.Use cross-validation to determine the optimal tree size. Which tree size corresponds to the lowest cross-validation error? Is this the same as the tree size obtained using the 1 SE rule?**

```{r}
set.seed(1)
data("Prostate")
ctrl <- trainControl(method = "cv")
```

```{r}
set.seed(1)
tree <- rpart(formula = lpsa~., data = Prostate,
               control = rpart.control(cp = 0.01))
cpTable <- printcp(tree)
plotcp(tree)
```
```{r}
minErr <- which.min(cpTable[,4])
minErr
```

**The tree size 8 corresponds to the lowest cross-validation error.**

```{r}
cpTable[cpTable[,4] < cpTable[minErr,4] + cpTable[minErr,5],1][1]  
```

**The tree size obtained using the 1 SE rule is 3.**

**(b) Create a plot of the final tree you choose. Pick one of the terminal nodes, and interpret the information displayed.**

```{r}
tree_a = prune(tree, cp = cpTable[cpTable[,4] < cpTable[minErr,4] + cpTable[minErr,5], 1][1])
rpart.plot(tree_a)
```

**The mean lpsa for observations with lcavol < 2.5 is 2.1. 78% of the observations have the mean lpsa 2.1.The mean lpsa for observations with lcavol < -0.48 is 0.6. 9% of the observations have the mean lpsa 0.6.**

**(c) Perform bagging and report the variable importance**

```{r}
bagging.grid <- expand.grid(mtry = 1:6,
                       splitrule = "variance",
                       min.node.size = 1:15)
set.seed(1)
bagging <- train(lpsa~., Prostate, 
                method = "ranger",
                tuneGrid = bagging.grid,
                trControl = ctrl,
                importance = "permutation")

ggplot(bagging, highlight = TRUE)
```

```{r}
barplot(sort(ranger::importance(bagging$finalModel), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```

**The important variables are : lcavol, lweight, svi**

**(d) Perform random forests and report the variable importance.**

```{r}
rf.grid = expand.grid(mtry = 1:6, 
                       splitrule = "variance",
                       min.node.size = 1:15) 
set.seed(1)
rf.fit = train(lpsa~., Prostate, 
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl,
                importance = 'permutation')
ggplot(rf.fit, highlight = TRUE)
```


```{r}
barplot(sort(ranger::importance(rf.fit$finalModel), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```

**The important variables are : lcavol, svi, lweight**

**(e) Perform boosting and report the variable importance.**

```{r}
gbm.grid <- expand.grid(n.trees = c(2000,3000),
                        interaction.depth = 2:10,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(1)
gbm.fit <- train(lpsa~., Prostate, 
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 trControl = ctrl,
                 verbose = FALSE)

ggplot(gbm.fit, highlight = TRUE)
```

**Variable importance from boosting can be obtained using the `summary()` function.**

```{r}
summary(gbm.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

**The important variables are : lcavol, lweight, svi**

**(f) Which of the above models will you select to predict PSA level? Explain.**

```{r}
resamp <- resamples(list(bagging = bagging, rf = rf.fit, gbm = gbm.fit))
summary(resamp)
bwplot(resamp, metric = "RMSE")
```

**I would choose the Random forest model since the mean RMSE is lowest for the rf model.**

**2. This problem involves the OJ data in the ISLR package. The data contains 1070 purchases where the customers either purchased Citrus Hill or Minute Maid Orange Juice. A number of characteristics of customers and products are recorded. Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations. Use set.seed() for reproducible results.**

```{r}
data("OJ")
set.seed(1)
rowTrain = createDataPartition(y = OJ$Purchase,
                                p = 0.747,
                                list = FALSE)
ctrl <- trainControl(method = "repeatedcv")
```

**(a) Fit a classification tree to the training set, with Purchase as the response and the other variables as predictors. Use cross-validation to determine the tree size and create a plot of the final tree. Predict the response on the test data. What is the test classification error rate?**

```{r}
set.seed(1)
rpart.class <- train(Purchase ~., OJ, 
                   subset = rowTrain,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-7,-2, len = 50))),
                   trControl = ctrl,
                   metric = "Accuracy")
ggplot(rpart.class, highlight = T)
rpart.plot(rpart.class$finalModel)
rpart.class$bestTune
```

**Predicting the response on the test data.**

```{r}
rpart.pred <- predict(rpart.class, newdata = OJ[-rowTrain,])
confusionMatrix(rpart.pred,
                reference = OJ$Purchase[-rowTrain])
```

```{r}
error_rate = mean(rpart.pred != OJ$Purchase[-rowTrain]) * 100
error_rate
```

**The test classification error rate is 17.407%**

**(b) Perform random forests on the training set and report variable importance. What is the test error rate?**

```{r}
rf.grid1 = expand.grid(mtry = 1:6, 
                       splitrule = "gini",
                       min.node.size = 1:6) 
set.seed(1)
rf.fit1 = train(Purchase~., OJ, 
                subset = rowTrain,
                method = "ranger",
                tuneGrid = rf.grid1,
                trControl = ctrl,
                importance = 'permutation')
ggplot(rf.fit1, highlight = TRUE)
```


```{r}
barplot(sort(ranger::importance(rf.fit1$finalModel), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```

**The important variables are LoyalCH**

```{r}
rf.pred <- predict(rf.fit1, newdata = OJ[-rowTrain,])
confusionMatrix(rf.pred,
                reference = OJ$Purchase[-rowTrain])
```

```{r}
error_rate_1 = mean(rf.pred != OJ$Purchase[-rowTrain]) * 100
error_rate_1
```

**The test error rate is 19.63%**

**(c) Perform boosting on the training set and report variable importance. What is the test error rate?**

```{r}
gbm2.grid <- expand.grid(n.trees = c(2000,3000,4000),
                        interaction.depth = 1:6,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(1)
gbm2.fit <- train(Purchase ~., OJ, 
                 subset = rowTrain, 
                 tuneGrid = gbm2.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "adaboost",
                 metric = "Accuracy",
                 verbose = FALSE)
ggplot(gbm2.fit, highlight = TRUE)
```

```{r}
summary(gbm2.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

**The important variable is LoyalCH**

```{r}
gbm2.pred <- predict(gbm2.fit, newdata = OJ[-rowTrain,])
confusionMatrix(gbm2.pred,
                reference = OJ$Purchase[-rowTrain])
```

```{r}
error_rate_2 = mean(gbm2.pred != OJ$Purchase[-rowTrain]) * 100
error_rate_2
```

**The test error rate is 16.29%**